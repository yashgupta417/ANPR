{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Augmentation Done!\n"
     ]
    }
   ],
   "source": [
    "#****************Data augmentation**************#\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "PATH='./character_dataset/Training Data'\n",
    "\n",
    "import os\n",
    "\n",
    "#listing all sub directories\n",
    "class_dirs=os.listdir(PATH)\n",
    "\n",
    "\n",
    "def augment(img,path,img_name):\n",
    "    \n",
    "    transforms=[\n",
    "        [A.Blur(blur_limit=(3,3),p=1)],#blur 1\n",
    "        [A.Blur(blur_limit=(5,5),p=1)],#blur 2\n",
    "        [A.Rotate(limit=(0,15),p=1)],#rotate the image clockwise\n",
    "        [A.Rotate(limit=(-15,0),p=1)],#rotate the image anti-clockwise\n",
    "        [A.Blur(blur_limit=(3,3),p=1),A.Rotate(limit=(-15,0),p=1)],#rotate anti-clockwise and blurr\n",
    "        [A.Blur(blur_limit=(3,3),p=1),A.Rotate(limit=(0,15),p=1)],#rotate clockwise and blurr\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    for i,t in enumerate(transforms):\n",
    "        \n",
    "        #defining transformation\n",
    "        transform=A.Compose(t)\n",
    "        \n",
    "        #applying transformation\n",
    "        aug_img=transform(image=img)['image']\n",
    "        \n",
    "        #augmented image name\n",
    "        aug_img_name=f'{path}/{img_name}_aug_{i}.jpg'\n",
    "        \n",
    "        #saving file\n",
    "        cv2.imwrite(aug_img_name,np.float32(aug_img))\n",
    "\n",
    "        \n",
    "#iterating over all directories\n",
    "for d in class_dirs:\n",
    "    \n",
    "    #listing all images in sub directory\n",
    "    files=os.listdir(os.path.join(PATH,d))\n",
    "    \n",
    "    #iterating over all images\n",
    "    for file in files:\n",
    "        \n",
    "        #reading image\n",
    "        img=cv2.imread(os.path.join(PATH,d,file))\n",
    "        img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        img_name=file.split('.')[0]\n",
    "        \n",
    "        #apply data augmentation\n",
    "        augment(img,os.path.join(PATH,d),img_name)\n",
    "        \n",
    "print(\"Data Augmentation Done!\")\n",
    "        \n",
    "def delete_augmentations():\n",
    "    #iterating over all directories\n",
    "    for d in class_dirs:\n",
    "\n",
    "        #listing all images in sub directory\n",
    "        files=os.listdir(os.path.join(PATH,d))\n",
    "\n",
    "        #iterating over all images\n",
    "        for file in files:\n",
    "            file_path=os.path.join(PATH,d,file)\n",
    "            if \"aug\" in file:\n",
    "                os.remove(file_path)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented data deleted\n"
     ]
    }
   ],
   "source": [
    "#***************************DANGER**************************#\n",
    "#****************For deleting augmentations*****************#\n",
    "delete_augmentations()\n",
    "print(\"Augmented data deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "864\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "\n",
    "#iterating over all directories\n",
    "for d in class_dirs:\n",
    "    \n",
    "    #listing all images in sub directory\n",
    "    files=os.listdir(os.path.join(PATH,d))\n",
    "    \n",
    "    #iterating over all images\n",
    "    for file in files:\n",
    "        \n",
    "        #reading image\n",
    "        img=cv2.imread(os.path.join(PATH,d,file))\n",
    "        \n",
    "        #binarizing image\n",
    "        img_gray=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img_bw=cv2.threshold(img_gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)[1]\n",
    "        \n",
    "        #flattening image and appending it\n",
    "        X.append(list(chain.from_iterable(img_bw)))\n",
    "        \n",
    "        #appending label\n",
    "        Y.append(file.split('_')[1])\n",
    "        \n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of images: 864\n",
      "no of dimensions: 784\n"
     ]
    }
   ],
   "source": [
    "print(f\"no of images: {len(X)}\")\n",
    "print(f\"no of dimensions: {len(X[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cc018ad970>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x0=np.reshape(X[0],(-1,28))\n",
    "plt.imshow(x0, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to numpy array\n",
    "X=np.array(X)\n",
    "\n",
    "#normalising X\n",
    "X=X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "#converting Y[i] to int values: 0-9 and 10-35(A-Z)\n",
    "for i in range(0,len(Y)):\n",
    "    if Y[i].isalpha():\n",
    "        Y[i]=ord(Y[i])-ord('A')+10\n",
    "    else:\n",
    "        Y[i]=int(Y[i])\n",
    "\n",
    "#one hot encoding\n",
    "Y=np_utils.to_categorical(Y,36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting to numpy array\n",
    "Y=np.array(Y)\n",
    "\n",
    "Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(864, 784)\n",
      "(864, 36)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim=36\n",
    "input_dim=784\n",
    "batch_size=32\n",
    "epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Perceptron instead of CNN or multi-layer-perceptrons since data size is really small (864 images)\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(output_dim,input_dim=input_dim,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "648\n",
      "216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=99)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21/21 [==============================] - 2s 31ms/step - loss: 3.6624 - accuracy: 0.1184 - val_loss: 2.1965 - val_accuracy: 0.5509\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 1.7498 - accuracy: 0.6952 - val_loss: 1.2001 - val_accuracy: 0.8194\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.9258 - accuracy: 0.8886 - val_loss: 0.7524 - val_accuracy: 0.8889\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.5755 - accuracy: 0.9374 - val_loss: 0.5407 - val_accuracy: 0.9213\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.3956 - accuracy: 0.9571 - val_loss: 0.4469 - val_accuracy: 0.9306\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.2961 - accuracy: 0.9783 - val_loss: 0.3672 - val_accuracy: 0.9491\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.2265 - accuracy: 0.9795 - val_loss: 0.3250 - val_accuracy: 0.9491\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.1894 - accuracy: 0.9839 - val_loss: 0.2925 - val_accuracy: 0.9583\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.1594 - accuracy: 0.9929 - val_loss: 0.2633 - val_accuracy: 0.9630\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.1345 - accuracy: 0.9974 - val_loss: 0.2442 - val_accuracy: 0.9630\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.1159 - accuracy: 0.9898 - val_loss: 0.2246 - val_accuracy: 0.9722\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.1097 - accuracy: 0.9941 - val_loss: 0.2209 - val_accuracy: 0.9630\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.0930 - accuracy: 0.9924 - val_loss: 0.1976 - val_accuracy: 0.9722\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.0789 - accuracy: 0.9962 - val_loss: 0.1970 - val_accuracy: 0.9676\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.0734 - accuracy: 0.9971 - val_loss: 0.1849 - val_accuracy: 0.9630\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.0744 - accuracy: 0.9980 - val_loss: 0.1756 - val_accuracy: 0.9722\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.9986 - val_loss: 0.1650 - val_accuracy: 0.9630\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9997 - val_loss: 0.1653 - val_accuracy: 0.9769\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9974 - val_loss: 0.1659 - val_accuracy: 0.9676\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9947 - val_loss: 0.1557 - val_accuracy: 0.9769\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,Y_train,batch_size=batch_size,epochs=epochs,validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 998us/step - loss: 0.1557 - accuracy: 0.9769\n",
      "Score 0.15568344295024872\n",
      "Accuracy 0.9768518805503845\n"
     ]
    }
   ],
   "source": [
    "pred=model.evaluate(X_test,Y_test)\n",
    "print(\"Score\",pred[0])\n",
    "print(\"Accuracy\",pred[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('char_recog_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_label(n):\n",
    "    if n<=9:\n",
    "        return str(n)\n",
    "    \n",
    "    return chr(n-10+ord('A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual: V\n",
      "pred: V\n"
     ]
    }
   ],
   "source": [
    "pred=model.predict(np.array([X_test[200]]))[0]\n",
    "\n",
    "# Generate arg maxes for prediction\n",
    "actual=np.argmax(Y_test[200])\n",
    "pred = np.argmax(pred)\n",
    "\n",
    "print(f\"actual: {to_label(actual)}\")\n",
    "print(f\"pred: {to_label(pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
